{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c84492",
   "metadata": {},
   "source": [
    "#### *ISEL - DEI - LEIM*\n",
    "## Aprendizagem Automática [T52D]\n",
    "### Trabalho Laboratorial 2: Classificação de Críticas de Cinema do IMDb\n",
    "\n",
    "João Madeira ($48630$), \n",
    "Renata Góis ($51038$),\n",
    "Bruno Pereira ($51811$)\n",
    "\n",
    "**Docentes responsáveis:** \n",
    "- Prof. Gonçalo Xufre Silva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0b3dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as p\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e196e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['data', 'target', 'DESCR'])\n",
      "50000 reviews\n"
     ]
    }
   ],
   "source": [
    "with open(\"resources/imdbFull.p\", \"rb\") as f:\n",
    "    D = p.load(f)\n",
    "print(\"Keys:\", D.keys())\n",
    "\n",
    "reviews = D['data']\n",
    "sentiments = D['target']\n",
    "\n",
    "print(len(reviews), \"reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b94ee",
   "metadata": {},
   "source": [
    "This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. This document outlines how the dataset was gathered, and how to use the files provided.\n",
    "For more details see: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4799e9cb",
   "metadata": {},
   "source": [
    "GPT answear when I asked what are my options for stemmers\n",
    "\n",
    "| Method                   | Aggressiveness | Quality | Speed  | Best for                    |\n",
    "| ------------------------ | -------------- | ------- | ------ | --------------------------- |\n",
    "| **Porter**               | Medium         | ✔✔      | Fast   | Classic NLP                 |\n",
    "| **Snowball**             | Medium         | ✔✔✔     | Fast   | Best stemmer for English    |\n",
    "| **Lancaster**            | High           | ✔       | Fast   | Rare cases; very aggressive |\n",
    "| **Lemmatizer (spaCy)**   | Low            | ⭐⭐⭐⭐    | Medium | Best accuracy               |\n",
    "| **Lemmatizer (WordNet)** | Low            | ⭐⭐⭐     | Medium | Simpler lemmatization       |\n",
    "\n",
    "So I opted for the \"Best stemmer for english\" since that what we are doing and for what I gathered Lemmatizer $ \\not= $ Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d2fdd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_review(string):\n",
    "    # Remove tags HTML\n",
    "    string = string.replace('<br />', ' ')  \n",
    "    # Remove palavras com 20 ou mais caracteres\n",
    "    string = re.sub(r'\\b[a-zA-Z]{20,}\\b', ' ', string)\n",
    "    # Remove palavras com 3 ou mais letras repetidas consecutivamente (e.g., \"yaaass\", \"omgggg\")\n",
    "    string = re.sub(r'\\b\\w*(.)\\1{2,}\\w*\\b', ' ', string)\n",
    "    # Filtra apenas letras\n",
    "    string = re.sub(r'[^a-zA-Z]', ' ', string)\n",
    "    # Remove espaços consecutivos\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    # Normaliza para minúsculas\n",
    "    string = string.lower()\n",
    "    # Aplica Stemming\n",
    "    string = \" \".join(stemmer.stem(w) for w in string.split())\n",
    "    return string\n",
    "\n",
    "reviews = [clean_review(rev) for rev in reviews]\n",
    "\n",
    "output = {\"data\" : reviews, \"target\" : sentiments}\n",
    "p.dump(output,open(\"resources/imdbPreProcessed.p\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d5aeff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = list(ENGLISH_STOP_WORDS - {'no', 'not', 'nor'})\n",
    "\n",
    "tfidVector = TfidfVectorizer(min_df=10,                    # Remove palavras que aparecem menos de 10 vezes no dataset\n",
    "                        max_df=0.9,                        # Remove palavras que aparecem em 90% do dataset \n",
    "                        max_features=70000,                # Limita o maximo de features para 30.000\n",
    "                        ngram_range=(1,2),                 # Utiliza unigramas e bigramas (good, very good, pretty bad)\n",
    "                        token_pattern=r'\\b[a-zA-Z]{2,}\\b', # Ignora palavras com menos de 2 letras\n",
    "                        sublinear_tf=True,                 # Term frequency passa a ter um comportamento logarítmico em vez de linear\n",
    "                        stop_words=custom_stopwords        # Remove stopwords em inglês excepto \"no\", \"not\" e \"nor\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e9140014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70000"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_data = p.load(open(\"resources/imdbPreProcessed.p\",\"rb\"))\n",
    "reviews = pre_processed_data['data']\n",
    "sentiments = pre_processed_data['target']\n",
    "\n",
    "tfidVector = tfidVector.fit(reviews)\n",
    "\n",
    "tokens = tfidVector.get_feature_names_out()\n",
    "X = tfidVector.transform(reviews)\n",
    "# X.astype(np.float32) # reduz para metade a utilização de RAM\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816f3f",
   "metadata": {},
   "source": [
    "### Divisão em conjuntos de treino, teste e validação\n",
    "| Treino | Teste | Validação |\n",
    "|--|--|--|\n",
    "|40k (80%)|5k (10%)|5k (10%)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "248db86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80% treino / 20% temporário (validação + teste)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, sentiments,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=sentiments\n",
    ")\n",
    "\n",
    "# Split 50% validação, 50% teste (10% / 10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "25d32ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'max_iter': 50, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['saga'],\n",
    "    'C': [0.1, 1, 2],\n",
    "    'max_iter': [50,100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42, n_jobs=1), param_grid,cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbd67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'C': 2, 'max_iter': 50, 'penalty': 'l2', 'solver': 'saga'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae910e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='saga', C=1, max_iter=50, random_state=42)\n",
    "lr = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7c8e21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "número de erros : 2799\n",
      "percentagem de acertos : 44.02%\n",
      "Matriz de confusão : \n",
      "[[858  33  40  39   4   7   2  29]\n",
      " [288  31  41  60   9  13   3  14]\n",
      " [208  20  72 108  19  23   4  42]\n",
      " [148  16  67 157  44  33   6  62]\n",
      " [ 27   4   9  46 118 120  20 136]\n",
      " [ 23   0   8  25  86 163  17 264]\n",
      " [ 14   0   5   9  42  77  24 290]\n",
      " [ 44   2   6   8  27  85  23 778]]\n"
     ]
    }
   ],
   "source": [
    "test_predicted = lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, test_predicted)\n",
    "print(f\"número de erros : {np.sum(test_predicted != y_test)}\")\n",
    "print(f\"percentagem de acertos : {np.round((np.sum(test_predicted == y_test)/y_test.shape[0])*100,2)}%\")\n",
    "print(f\"Matriz de confusão : \\n{cm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fcaa36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

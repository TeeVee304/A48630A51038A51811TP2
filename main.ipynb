{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40c84492",
   "metadata": {},
   "source": [
    "#### *ISEL - DEI - LEIM*\n",
    "## Aprendizagem Automática [T52D]\n",
    "### Trabalho Laboratorial 2: Classificação de Críticas de Cinema do IMDb\n",
    "\n",
    "João Madeira ($48630$), \n",
    "Renata Góis ($51038$),\n",
    "Bruno Pereira ($51811$)\n",
    "\n",
    "**Docentes responsáveis:** \n",
    "- Prof. Gonçalo Xufre Silva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0b3dd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as p\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import sklearn.preprocessing as pp\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e196e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: dict_keys(['data', 'target', 'DESCR'])\n",
      "50000 reviews\n"
     ]
    }
   ],
   "source": [
    "with open(\"resources/imdbFull.p\", \"rb\") as f:\n",
    "    D = p.load(f)\n",
    "print(\"Keys:\", D.keys())\n",
    "\n",
    "reviews = D['data']\n",
    "sentiments = D['target']\n",
    "\n",
    "print(len(reviews), \"reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7713693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_stats(target, predicted):\n",
    "    cm = confusion_matrix(target, predicted)\n",
    "    accuracy = accuracy_score(target, predicted)\n",
    "    precision = precision_score(target, predicted,average=\"macro\")\n",
    "    recall = recall_score(target, predicted,average=\"macro\")\n",
    "\n",
    "    print(\"Conjunto de Teste\")\n",
    "    print(\"Accuracy:\", np.round(accuracy,2))\n",
    "    print(\"Precision:\", np.round(precision,2))\n",
    "    print(\"Recall:\", np.round(recall,2))\n",
    "    print(f\"Matriz de confusão : \\n\",cm)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76b94ee",
   "metadata": {},
   "source": [
    "This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification. This document outlines how the dataset was gathered, and how to use the files provided.\n",
    "For more details see: http://ai.stanford.edu/~amaas/data/sentiment/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d05b6a",
   "metadata": {},
   "source": [
    "### **1.** Pré-processamento e Limpeza de Texto\n",
    "O objetivo desta etapa foi reduzir o ruído e a dimensionalidade do vocabulário, mantendo o conteúdo semântico relevante.\n",
    "\n",
    "**Stemming:** Aplicou-se o algoritmo `SnowballStemmer` para reduzir as palavras à sua raiz lexical, permitindo agrupar variações da mesma palavra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d2fdd91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def clean_review(string):\n",
    "    # Remove tags HTML\n",
    "    string = string.replace('<br />', ' ')  \n",
    "    # Remove palavras com 20 ou mais caracteres\n",
    "    string = re.sub(r'\\b[a-zA-Z]{20,}\\b', ' ', string)\n",
    "    # Remove palavras com 3 ou mais letras repetidas consecutivamente (e.g., \"yaaass\", \"omgggg\")\n",
    "    string = re.sub(r'\\b\\w*(.)\\1{2,}\\w*\\b', ' ', string)\n",
    "    # Filtra apenas letras\n",
    "    string = re.sub(r'[^a-zA-Z]', ' ', string)\n",
    "    # Remove espaços consecutivos\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "    # Normaliza para minúsculas\n",
    "    string = string.lower()\n",
    "    # Aplica Stemming\n",
    "    string = \" \".join(stemmer.stem(w) for w in string.split())\n",
    "    return string\n",
    "\n",
    "reviews = [clean_review(rev) for rev in reviews]\n",
    "\n",
    "output = {\"data\" : reviews, \"target\" : sentiments}\n",
    "p.dump(output,open(\"resources/imdbPreProcessed.p\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1ea99",
   "metadata": {},
   "source": [
    "### **2.** Construção do Vocabulário (TF-IDF)\n",
    "Configurou-se o `TfidfVectorizer` da seguinte forma:\n",
    "- **N-grams (1,2):** Incluíram-se bigramas para capturar contexto local (ex: \"*not good*\").\n",
    "\n",
    "- **Stop-words:** Personalizou-se a lista de exclusão para **manter** termos de negação (como \"*no*\" e \"*not*\"), necessários para a inversão de polaridade, que seriam removidos por defeito.\n",
    "\n",
    "- **Limites:** Definiram-se `min_df=3` e `max_features=30k` para eliminar erros ortográficos pontuais e controlar o uso de memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5aeff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = list(ENGLISH_STOP_WORDS - {'no', 'not', 'nor'})\n",
    "\n",
    "tfidfVector = TfidfVectorizer(min_df=3,                    # Remove palavras que aparecem menos de 10 vezes no dataset\n",
    "                        max_df=0.8,                        # Remove palavras que aparecem em 90% do dataset \n",
    "                        max_features=30000,                # Limita o maximo de features para 30.000\n",
    "                        ngram_range=(1,2),                 # Utiliza unigramas e bigramas (good, very good, pretty bad)\n",
    "                        token_pattern=r'\\b[a-zA-Z]{2,}\\b', # Ignora palavras com menos de 2 letras\n",
    "                        sublinear_tf=True,                 # Term frequency passa a ter um comportamento logarítmico em vez de linear\n",
    "                        stop_words=custom_stopwords        # Remove stopwords em inglês excepto \"no\", \"not\" e \"nor\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9140014",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_processed_data = p.load(open(\"resources/imdbPreProcessed.p\",\"rb\"))\n",
    "reviews = pre_processed_data['data']\n",
    "sentiments = pre_processed_data['target']\n",
    "\n",
    "tfidfVector = tfidfVector.fit(reviews)\n",
    "\n",
    "tokens = tfidfVector.get_feature_names_out()\n",
    "X = tfidfVector.transform(reviews)\n",
    "# X.astype(np.float32) # reduz para metade a utilização de RAM\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e816f3f",
   "metadata": {},
   "source": [
    "### **3.** Divisão em conjuntos de treino, teste e validação\n",
    "Realizou-se a partição do *dataset* em três subconjuntos: **treino**, **teste** e **validação**. Utilizou-se amostragem estratificada (`stratify`) para garantir que a distribuição original das classes de *rating* ($1$ a $10$) fosse preservada proporcionalmente em todos os subconjuntos.\n",
    "\n",
    "| Treino | Teste | Validação |\n",
    "|--|--|--|\n",
    "|40k (80%)|5k (10%)|5k (10%)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "248db86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 80% treino / 20% temporário (validação + teste)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, sentiments,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=sentiments\n",
    ")\n",
    "\n",
    "# Split 50% validação, 50% teste (10% / 10%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0955553f",
   "metadata": {},
   "source": [
    "### **4.** Seleção de Modelo e Hiperparâmetros\n",
    "Optou-se por testar os modelos **LogisticRegression**, **LinearSVC** e **Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be41e97",
   "metadata": {},
   "source": [
    "### LogisticRegression\n",
    "\n",
    "* **Otimização:** Efetuou-se uma pesquisa em grelha (`GridSearchCV`) com validação cruzada (`cv=3`) para determinar o valor ideal do parâmetro de regularização `C`.\n",
    "\n",
    "* **Regularização:** Manteve-se a penalização L2 para mitigar o risco de *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "25d32ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'max_iter': 50, 'penalty': 'l2', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['l2'],\n",
    "    'solver': ['saga'],\n",
    "    'C': [1, 2, 5],\n",
    "    'max_iter': [50,100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(random_state=42, n_jobs=1), param_grid,cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ae910e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty='l2', solver='saga', C=1, max_iter=50, random_state=42)\n",
    "lr = lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68a4e0",
   "metadata": {},
   "source": [
    "### Análise de Desempenho\n",
    "Avaliou-se o classificador com os conjuntos de Teste e Validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e21aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Teste\n",
      "número de erros : 2836\n",
      "percentagem de acertos : 43.28%\n",
      "Matriz de confusão : \n",
      "[[828  47  47  40   7  10   3  30]\n",
      " [278  28  50  63   8  11   3  18]\n",
      " [200  23  81 102  17  27   7  39]\n",
      " [140  24  65 163  47  22   7  65]\n",
      " [ 28   6  11  41 121 124  19 130]\n",
      " [ 19   4  10  31  98 159  21 244]\n",
      " [ 10   1   6   8  38  91  31 276]\n",
      " [ 46   1   8   5  36  95  29 753]]\n"
     ]
    }
   ],
   "source": [
    "predicted_lrtest = lr.predict(X_test)\n",
    "\n",
    "class_stats(y_test, predicted_lrtest, lr.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c0fe95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Validação\n",
      "número de erros : 2832\n",
      "percentagem de acertos : 43.36%\n",
      "Matriz de confusão : \n",
      "[[817  34  50  56   6  10   5  34]\n",
      " [266  35  42  72  10   9   2  22]\n",
      " [189  39  83 114  23  16   5  27]\n",
      " [129  19  71 188  51  30   7  38]\n",
      " [ 29   8  21  41 129 111  15 127]\n",
      " [ 26   5   6  26 106 126  39 252]\n",
      " [ 13   2   6   8  33  87  35 277]\n",
      " [ 40   2   5  16  32  93  30 755]]\n"
     ]
    }
   ],
   "source": [
    "predicted_lrval = lr.predict(X_val)\n",
    "\n",
    "class_stats(y_val, predicted_lrval, lr.predict_proba(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6674624",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6fcb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 2, 5],\n",
    "    'max_iter': [2000, 5000, 10000]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LinearSVC(random_state=42), param_grid,cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3160cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(random_state=42, C=0.1, max_iter=2000)\n",
    "lsvc = lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0191ad15",
   "metadata": {},
   "source": [
    "### Análise de Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d50bc5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Teste\n",
      "Accuracy: 0.43\n",
      "Precision: 0.34\n",
      "Recall: 0.33\n",
      "Matriz de confusão : \n",
      " [[869  25  32  32   5   9   3  37]\n",
      " [312  22  31  55   8  10   2  19]\n",
      " [228  21  61  95  21  18   5  47]\n",
      " [172  18  55 151  38  22   8  69]\n",
      " [ 30   6  12  40 108 108  15 161]\n",
      " [ 24   3   7  28  90 122  14 298]\n",
      " [ 17   1   1   7  34  63  24 314]\n",
      " [ 44   1   5   7  29  70  18 799]]\n"
     ]
    }
   ],
   "source": [
    "predicted_svctest = lsvc.predict(X_test)\n",
    "\n",
    "class_stats(y_test, predicted_svctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1ab884a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Teste\n",
      "Accuracy: 0.44\n",
      "Precision: 0.34\n",
      "Recall: 0.33\n",
      "Matriz de confusão : \n",
      " [[864  24  27  39   6   9   3  40]\n",
      " [307  20  25  62   9   6   1  28]\n",
      " [231  22  71 108  17  13   4  30]\n",
      " [150  17  67 172  44  23   8  52]\n",
      " [ 32   4  14  40 119 107   7 158]\n",
      " [ 30   3   9  22  96 115  23 288]\n",
      " [ 15   2   6  10  28  67  21 312]\n",
      " [ 36   1   3  12  27  67  22 805]]\n"
     ]
    }
   ],
   "source": [
    "predicted_svcval = lsvc.predict(X_val)\n",
    "\n",
    "class_stats(y_val, predicted_svcval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4e2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None],\n",
    "    \"min_samples_split\": [2]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid,cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "# não foi possivel obter um resultado com o grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d15ef845",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42, n_estimators= 200, max_depth= None, min_samples_split= 2)\n",
    "rfc = rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5e3160",
   "metadata": {},
   "source": [
    "### Análise de Desempenho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3c207ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Teste\n",
      "Accuracy: 0.38\n",
      "Precision: 0.46\n",
      "Recall: 0.25\n",
      "Matriz de confusão : \n",
      " [[916   0   2   6   2   5   0  81]\n",
      " [382   5   2   5   2   3   0  60]\n",
      " [346   0  12  18   2   5   0 113]\n",
      " [302   0   9  29   5  14   0 174]\n",
      " [101   0   1  14  28  37   0 299]\n",
      " [ 81   0   4   7  15  42   0 437]\n",
      " [ 38   0   1   6   5  23   2 386]\n",
      " [ 83   0   0   2   7  19   2 860]]\n"
     ]
    }
   ],
   "source": [
    "predicted_rfctest = rfc.predict(X_test)\n",
    "\n",
    "class_stats(y_test, predicted_rfctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49e1c0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de Teste\n",
      "Accuracy: 0.39\n",
      "Precision: 0.53\n",
      "Recall: 0.26\n",
      "Matriz de confusão : \n",
      " [[917   0   1   5   0   3   0  86]\n",
      " [363   6   4  10   0   1   0  74]\n",
      " [365   0  17  17   3   6   0  88]\n",
      " [323   0   6  41   8  18   0 137]\n",
      " [ 94   0   4  18  30  49   1 285]\n",
      " [ 95   0   2   7  16  49   0 417]\n",
      " [ 38   0   0   2   7  24   8 382]\n",
      " [ 84   0   0   3   6  22   0 858]]\n"
     ]
    }
   ],
   "source": [
    "predicted_rfctest = rfc.predict(X_val)\n",
    "\n",
    "class_stats(y_val, predicted_rfctest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584f327",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
